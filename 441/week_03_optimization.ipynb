{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118c96d8",
   "metadata": {},
   "source": [
    "\n",
    "# Week 3: Optimization Techniques in Neural Networks\n",
    "## Objective:\n",
    "In this notebook, we will explore various optimization techniques used in deep learning, such as Gradient Descent (GD), Stochastic Gradient Descent (SGD), and Mini-Batch Gradient Descent. We will understand how these methods work and implement them in Python to optimize a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89f38a",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Theory: Introduction to Optimization\n",
    "Optimization is the process of minimizing a loss function by updating the model's parameters (weights and biases). In neural networks, the most common optimization technique is **Gradient Descent (GD)**, which iteratively adjusts weights to minimize the error.\n",
    "\n",
    "### Key Optimization Methods:\n",
    "- **Gradient Descent (GD)**: Updates weights using the entire dataset in each iteration. It is computationally expensive for large datasets.\n",
    "- **Stochastic Gradient Descent (SGD)**: Updates weights using a single data point in each iteration, making it faster but more noisy.\n",
    "- **Mini-Batch Gradient Descent**: Combines the advantages of GD and SGD by updating weights using small batches of data, balancing speed and accuracy.\n",
    "\n",
    "### Learning Rate:\n",
    "The **learning rate** controls how much the model's weights are adjusted in response to the gradient. Choosing an appropriate learning rate is crucial for model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple Gradient Descent Implementation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Update weights and bias\n",
    "        weights -= lr * (1/m) * np.dot(X.T, error)\n",
    "        bias -= lr * (1/m) * np.sum(error)\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([5, 7, 9, 11])\n",
    "\n",
    "# Running Gradient Descent\n",
    "weights, bias = gradient_descent(X, y)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb2187",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Stochastic Gradient Descent (SGD)\n",
    "Unlike Gradient Descent, which updates weights using the entire dataset, **SGD** updates the weights using only a single data point at a time. This makes it faster, especially for large datasets, but introduces more noise in the weight updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stochastic Gradient Descent Implementation\n",
    "\n",
    "def stochastic_gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(m):\n",
    "            y_pred = np.dot(X[i], weights) + bias\n",
    "            error = y_pred - y[i]\n",
    "            \n",
    "            # Update weights and bias\n",
    "            weights -= lr * error * X[i]\n",
    "            bias -= lr * error\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "# Running SGD\n",
    "weights_sgd, bias_sgd = stochastic_gradient_descent(X, y)\n",
    "print(\"Weights (SGD):\", weights_sgd)\n",
    "print(\"Bias (SGD):\", bias_sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64232720",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 Mini-Batch Gradient Descent\n",
    "**Mini-Batch Gradient Descent** is a compromise between GD and SGD. It divides the dataset into small batches and performs weight updates on each batch. This approach provides a balance between the stability of GD and the speed of SGD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee639284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mini-Batch Gradient Descent Implementation\n",
    "\n",
    "def mini_batch_gradient_descent(X, y, lr=0.01, epochs=1000, batch_size=2):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            y_pred = np.dot(X_batch, weights) + bias\n",
    "            error = y_pred - y_batch\n",
    "            \n",
    "            # Update weights and bias\n",
    "            weights -= lr * (1/batch_size) * np.dot(X_batch.T, error)\n",
    "            bias -= lr * (1/batch_size) * np.sum(error)\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "# Running Mini-Batch Gradient Descent\n",
    "weights_mini_batch, bias_mini_batch = mini_batch_gradient_descent(X, y)\n",
    "print(\"Weights (Mini-Batch GD):\", weights_mini_batch)\n",
    "print(\"Bias (Mini-Batch GD):\", bias_mini_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f007d",
   "metadata": {},
   "source": [
    "\n",
    "## 3.4 Exercises:\n",
    "- Compare the performance of GD, SGD, and Mini-Batch GD by testing them on different datasets.\n",
    "- Experiment with different learning rates and observe their effect on the speed of convergence.\n",
    "- Use a larger dataset and compare how fast each optimization method converges to the solution.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
