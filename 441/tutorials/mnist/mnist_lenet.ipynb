{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a6d490-20bd-4a04-a895-9a5fa09c76a0",
   "metadata": {},
   "source": [
    "### **MNIST Classification Tutorial using LeNet Architecture**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Overview**\n",
    "\n",
    "The **LeNet architecture**, developed by Yann LeCun in 1998, is one of the first convolutional neural networks (CNNs) and was designed for image classification tasks like recognizing handwritten digits in the MNIST dataset.\n",
    "\n",
    "In this tutorial, we will build a convolutional neural network (CNN) based on the **LeNet** architecture to classify handwritten digits from the MNIST dataset. We'll cover the following topics:\n",
    "\n",
    "1. **Data Loading**: Loading and preprocessing the MNIST dataset.\n",
    "2. **Model Definition**: Defining the LeNet architecture for image classification.\n",
    "3. **Training**: Using an optimizer and a loss function to train the model.\n",
    "4. **Evaluation**: Assessing the model's performance on the test set.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Loading**\n",
    "\n",
    "We will use the `torchvision` library to load and preprocess the MNIST dataset. The dataset contains 28x28 grayscale images.\n",
    "\n",
    "### **Code:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Model Definition (LeNet)**\n",
    "\n",
    "The **LeNet** architecture consists of the following layers:\n",
    "\n",
    "1. **Convolutional Layer 1**: Input (1x28x28) → Output (6x24x24)\n",
    "2. **Pooling Layer 1**: Input (6x24x24) → Output (6x12x12)\n",
    "3. **Convolutional Layer 2**: Input (6x12x12) → Output (16x8x8)\n",
    "4. **Pooling Layer 2**: Input (16x8x8) → Output (16x4x4)\n",
    "5. **Fully Connected Layer 1**: Flattened input (16x4x4 = 256) → 120 neurons\n",
    "6. **Fully Connected Layer 2**: 120 neurons → 84 neurons\n",
    "7. **Fully Connected Layer 3**: 84 neurons → 10 output neurons (one for each digit)\n",
    "\n",
    "### **Code:**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # Convolutional Layer 1\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Pooling Layer\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # Convolutional Layer 2\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Fully Connected Layer 1\n",
    "        self.fc2 = nn.Linear(120, 84)  # Fully Connected Layer 2\n",
    "        self.fc3 = nn.Linear(84, 10)  # Fully Connected Layer 3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 -> ReLU -> Pool\n",
    "        x = x.view(-1, 16 * 4 * 4)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))  # Fully connected layer 1\n",
    "        x = F.relu(self.fc2(x))  # Fully connected layer 2\n",
    "        x = self.fc3(x)  # Fully connected layer 3\n",
    "        return F.log_softmax(x, dim=1)  # Log-Softmax for classification\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Training the Model**\n",
    "\n",
    "### **Loss Function:**\n",
    "\n",
    "For multi-class classification, we use the negative log-likelihood loss (`torch.nn.NLLLoss`).\n",
    "\n",
    "### **Optimizer:**\n",
    "\n",
    "We will use the Adam optimizer for training.\n",
    "\n",
    "### **Training Loop:**\n",
    "\n",
    "We'll train the model for a specified number of epochs, updating the weights after each mini-batch.\n",
    "\n",
    "### **Code:**\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = LeNet()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Evaluation**\n",
    "\n",
    "After training the model, we evaluate its performance on the test dataset to see how well it generalizes.\n",
    "\n",
    "### **Code:**\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In this tutorial, we built a convolutional neural network based on the **LeNet** architecture to classify handwritten digits from the MNIST dataset. We trained the model using the Adam optimizer and evaluated its performance on the test set.\n",
    "\n",
    "You can further improve the model by experimenting with different hyperparameters, data augmentation techniques, or using advanced architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47761d8b-dd95-4b02-9d4d-f19a145131e8",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "#### **Architecture Summary**:\n",
    "- **Input Layer**: Accepts grayscale images (1x28x28).\n",
    "- **Convolutional Layer 1 (Conv1)**: Applies 6 convolutional filters (kernels) to the input image.\n",
    "- **Pooling Layer 1**: Reduces the spatial dimensions by down-sampling the feature maps.\n",
    "- **Convolutional Layer 2 (Conv2)**: Applies 16 convolutional filters to the feature maps produced by the first pooling layer.\n",
    "- **Pooling Layer 2**: Further down-samples the feature maps.\n",
    "- **Fully Connected Layers**: Three fully connected layers are used for classification.\n",
    "\n",
    "#### **LeNet Architecture Breakdown**:\n",
    "1. **Input**: 1x28x28 (grayscale image).\n",
    "2. **Conv1**: 6 filters of size 5x5 → Output size: 6x24x24.\n",
    "3. **Max Pooling**: 2x2 pooling → Output size: 6x12x12.\n",
    "4. **Conv2**: 16 filters of size 5x5 → Output size: 16x8x8.\n",
    "5. **Max Pooling**: 2x2 pooling → Output size: 16x4x4.\n",
    "6. **Fully Connected Layer 1**: Input size 16x4x4 = 256 → 120 neurons.\n",
    "7. **Fully Connected Layer 2**: 120 → 84 neurons.\n",
    "8. **Output Layer**: 84 → 10 neurons (for 10 digit classes).\n",
    "\n",
    "### **2. Kernel Size**\n",
    "- **What is Kernel Size?**\n",
    "  - The **kernel** (or filter) in a convolutional layer is a small matrix of weights that slides over the input image, computing dot products with overlapping sections of the input.\n",
    "  - The **kernel size** refers to the dimensions of this matrix.\n",
    "  \n",
    "- **In LeNet**:\n",
    "  - Both convolutional layers in LeNet use a **kernel size of 5x5**, meaning each filter covers a 5x5 region of the input.\n",
    "\n",
    "- **Effect of Kernel Size**:\n",
    "  - Larger kernels capture more spatial features but require more computations and parameters.\n",
    "  - Smaller kernels are computationally cheaper but capture less complex patterns.\n",
    "\n",
    "### **3. Number of Kernels (Filters)**\n",
    "- **What is the Number of Kernels?**\n",
    "  - The number of kernels (or filters) determines how many different feature maps are learned at each convolutional layer. Each kernel extracts a different feature from the input.\n",
    "\n",
    "- **In LeNet**:\n",
    "  - **Conv1**: 6 kernels (filters) are applied, producing 6 different feature maps.\n",
    "  - **Conv2**: 16 kernels are applied, producing 16 feature maps.\n",
    "\n",
    "- **Effect of Number of Kernels**:\n",
    "  - More kernels allow the network to learn more diverse features but increase the computational load and memory requirements.\n",
    "  - Too few kernels may lead to underfitting, as the model may not capture enough complexity in the data.\n",
    "\n",
    "### **4. Padding**\n",
    "- **What is Padding?**\n",
    "  - **Padding** refers to adding zeros around the input image before applying the convolution. This technique controls the spatial size of the output feature maps.\n",
    "  - Padding can prevent the reduction in the spatial dimensions of the input as it passes through the convolutional layers.\n",
    "\n",
    "- **In LeNet**:\n",
    "  - No padding is used in the original LeNet architecture, which is why the size of the feature maps decreases after each convolution.\n",
    "    - **Conv1 Input**: 28x28 → No padding → **Conv1 Output**: 24x24.\n",
    "    - **Conv2 Input**: 12x12 → No padding → **Conv2 Output**: 8x8.\n",
    "\n",
    "- **Effect of Padding**:\n",
    "  - **Same padding** (padding to maintain the input size) preserves the spatial dimensions after convolution.\n",
    "  - **Valid padding** (no padding) reduces the spatial dimensions, as seen in LeNet.\n",
    "\n",
    "### **5. Stride**\n",
    "- **What is Stride?**\n",
    "  - **Stride** defines how much the filter moves at each step when sliding over the input. A stride of 1 means the filter moves 1 pixel at a time, while a stride of 2 skips every other pixel.\n",
    "\n",
    "- **In LeNet**:\n",
    "  - Both convolutional layers in LeNet use a stride of **1**, meaning the filter moves one pixel at a time.\n",
    "  - The pooling layers use a **stride of 2**, meaning the pooling operation reduces the spatial dimensions by half.\n",
    "\n",
    "- **Effect of Stride**:\n",
    "  - A larger stride reduces the size of the output feature maps faster, resulting in a lower spatial resolution.\n",
    "  - Stride controls the trade-off between computational efficiency and the level of detail in the feature maps.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "In Week 04, we will cover how LeNet works, focusing on these architectural components:\n",
    "- **Kernel Size**: Determines the receptive field of each filter and what features are captured.\n",
    "- **Number of Kernels**: Controls the number of different features learned in each convolutional layer.\n",
    "- **Padding**: Affects the spatial dimensions of the output and whether the borders of the input are preserved.\n",
    "- **Stride**: Controls how much the filter shifts across the input and the size of the output feature maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5e42b-0ebe-4ae1-a490-e67eacb9daaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
